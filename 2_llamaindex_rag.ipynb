{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Leverage LLamaIndex with VertexAI Vector Search to perform question answering RAG\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will go over how to create a RAG framework using LlamaIndex and VertexAI Vector Search effectively.\n",
    "\n",
    "LlamaIndex is used to parse, chunk, and embed the input data using Gemini Text Embedding models. We use store the parsed data in a VertexAI Vector Search index that will searched against during inference to retrieve context to augment prompts for question answering task.\n",
    "\n",
    "### Objectives\n",
    "This notebook provides a guide to building a questions answering system using retrieval augmented generation (RAG) framework that leverages LLamaIndex for data ingestion and Vector Store creation.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Set up Google Cloud resources required: GCS Bucket and Vertex AI Vector Search index and deployed endpoint\n",
    "2. Ingest, parse, chunk, and embed data using LlamaIndex with Gemini Text Embedding models.\n",
    "3. Search the vector store with an incoming text queries to find similar text data that can be used as context in the prompt\n",
    "4. Generate answer to the user query using Gemini Pro Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Install any dependencies that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index \\\n",
    "  llama-index-embeddings-vertex \\\n",
    "  llama-index-llms-vertex \\\n",
    "  llama-index-vector_stores-vertexaivectorsearch \\\n",
    "  langchain-community \\\n",
    "  termcolor \\\n",
    "  llama-index-llms-langchain \\\n",
    "  llama-index-llms-fireworks \\\n",
    "  langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.core.prompts import LangchainPromptTemplate\n",
    "from langchain import hub\n",
    "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\" #TODO - add your project-id here from the console\n",
    "REGION = \"\"  #TODO - add your region here from the console\n",
    "GCS_BUCKET = \"llamaindex_gcs_bucket\"  # @param {type:\"string\"}\n",
    "VS_INDEX_NAME = \"llamaindex_doc_index\"  # @param {type:\"string\"}\n",
    "VS_INDEX_ENDPOINT_NAME = \"llamaindex_doc_endpoint\"  # @param {type:\"string\"}\n",
    "DOC_FOLDER = \"./data\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate sample data\n",
    "\n",
    "Refer to document 04a02.pdf\n",
    "\n",
    "This document describes the importance of stable power grids in Japan, highlighting the recent failure of a generator step-up transformer at the Nakoso Power Station and the rapid restoration response undertaken to maintain power supply stability.\n",
    "\n",
    "We will use this pdf moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data using Llama-Index into VertexAI Vector Search\n",
    "\n",
    "The following section leverages LlamaIndex to ingest, chunk, and embed the PDF data to be connected to the VertexAI Vector Store.\n",
    "\n",
    "At the end of this section you will be ready to query against the Vector Store to find relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm_and_storage(vs_index, vs_endpoint):\n",
    "    \"\"\"\n",
    "    Initializes VertexAI Vector Store given a VertexAI Search index and deployed endpoint.\n",
    "    Configures embedding and LLMs models to be gecko and Gemini.\n",
    "    \"\"\"\n",
    "    # setup storage\n",
    "    vector_store = VertexAIVectorStore(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        index_id=vs_index,\n",
    "        endpoint_id=vs_endpoint,\n",
    "        gcs_bucket_name=GCS_BUCKET,\n",
    "    )\n",
    "\n",
    "    # set storage context\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    gemini_embedding_model = VertexTextEmbedding(\"text-embedding-004\")\n",
    "    llm = Vertex(\"gemini-pro\")\n",
    "\n",
    "    Settings.embed_model = gemini_embedding_model\n",
    "    Settings.llm=llm\n",
    "\n",
    "    return storage_context\n",
    "\n",
    "def ingest_document():\n",
    "    '''\n",
    "    Using SimpleDirectoryReader, which creates documents out of every file in a given directory. It is built in to LlamaIndex and can read a variety of formats including Markdown, PDFs, Word documents, PowerPoint decks, images, audio and video.\n",
    "    '''\n",
    "    documents = SimpleDirectoryReader(input_files=[DOC_FOLDER+\"/04a02.pdf\"]).load_data()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a LlamaIndex retriever on VertexAI using the resources created in the gcp.setup.ipynb notebook.\n",
    "\n",
    "Copy and paste the values from the previous notebook  in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_index_id = \"\" #TODO - add your vertexai search id from setup here\n",
    "vs_endpoint_id = \"\"  #TODO - add your vertexai search deployed endpoint id from setup here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = initialize_llm_and_storage(vs_index_id, vs_endpoint_id)\n",
    "docs = ingest_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see the documents that were parsed using LlamaIndex\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Q/A RAG\n",
    "\n",
    "This section peforms the RAG prompt and returns an answer to the user query. To explore RAG frameworks we will look at 3 different options:\n",
    "\n",
    "1. Using the built-in RAG prompt provided by LlamaIndex\n",
    "2. Connecting a LangChain RAG template\n",
    "3. Creating a custom few-shot example RAG template.\n",
    "\n",
    "For each option, you will see the current text prompt structure and the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up helper functions\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    \"\"\"\n",
    "    Used to display the underlying text prompt used for RAG.\n",
    "    \"\"\"\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        print(text_md)\n",
    "        print(p.get_template())\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "def display_and_run_prompt(query_engine, query_str, show_prompt = True):\n",
    "    \"\"\"\n",
    "    Displays the current RAG prompt used and runs the query against the RAG workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    if show_prompt:\n",
    "        print(\"----Displaying current prompt dictionary----\\n\")\n",
    "        prompts_dict = query_engine.get_prompts()\n",
    "        display_prompt_dict(prompts_dict)\n",
    "\n",
    "    response = query_engine.query(query_str)\n",
    "    print(f\"Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(response.response)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Source Documents:\")\n",
    "    print(\"-\" * 80)\n",
    "    for source in response.source_nodes:\n",
    "        print(f\"Sample Text: {source.text[:200]}\")\n",
    "        print(f\"Relevance score: {source.get_score():.3f}\")\n",
    "        print(f\"File Name: {source.metadata.get('file_name')}\")\n",
    "        print(f\"Page #: {source.metadata.get('page_label')}\")\n",
    "        print(f\"File Path: {source.metadata.get('file_path')}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlamaIndex Built-in RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_built_in_prompt(query_engine, query_str, show_prompt = True):\n",
    "    display_and_run_prompt(query_engine, query_str, show_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templeted RAG through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_rag_prompt(query_engine, query_str, show_prompt = True):\n",
    "    langchain_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    langchain_prompt_template = LangchainPromptTemplate(\n",
    "        template=langchain_prompt,\n",
    "        template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
    "    )\n",
    "\n",
    "    query_engine.update_prompts(\n",
    "        {\"response_synthesizer:text_qa_template\": langchain_prompt_template}\n",
    "    )\n",
    "\n",
    "    display_and_run_prompt(query_engine, query_str, show_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom RAG Implementation\n",
    "\n",
    "This custom RAG prompt highlights two important prompt engineering techniques:\n",
    "\n",
    "__Few-shot examples:__ Providing the model with a few examples of the desired input-output behavior helps guide the model's response, effectively demonstrating the task and the expected format. This is particularly useful when the task is complex or requires a specific style of output.\n",
    "\n",
    "__Grounding the output:__ Instructing the model to base its answer on the retrieved documents and to provide justification for the answer ensures that the response is factually grounded and relevant to the context. This is crucial for maintaining accuracy and preventing the model from generating responses that are either irrelevant or factually incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_few_shot_prompt(query_engine, query_str, show_prompt = True):\n",
    "    \"\"\"\n",
    "    Generating custom few shot prompt to show the desired output format and prevent hallucination by including reasoning in the response.\n",
    "    \"\"\"\n",
    "\n",
    "    qa_prompt_custom_string= \"\"\"\\\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query asking about citations over different topics.\n",
    "\n",
    "    Please output your answer in the following JSON format:\n",
    "    JSON Output: [\n",
    "    \"answer\": 'This is the answer to the question',\n",
    "    \"justification\": 'This is the reasoning or evidence supporting the answer given the provided context'\n",
    "    ]\n",
    "\n",
    "    Example query and JSON output:\n",
    "    Query: Who are the authors of the paper?\n",
    "    JSON Output: [\n",
    "    \"answer\": \"The authors are Hikaru Fujita, Masaru Kashiwakura, Akihiro Kawagoe, Hisaki Hamamoto, Tetsuo Niitsuma, and Yuzuru Mitani.\"\n",
    "    \"justification\": \"The authors are listed on the first and last page in order.\"\n",
    "    ]\n",
    "\n",
    "    Query: When was there a failure at the Nakoso Power Station?\n",
    "    JSON Output: [\n",
    "    \"answer\": \"September  16,  2021\",\n",
    "    \"justification\": \"In the context provided it states: It was in this context\n",
    "    that  on  September  16,  2021,  a  failure  due  to  aging  forced  the  emergency  stop  of  the\n",
    "    unit No. 8 generator step-up transformer (built in 1981) at the Nakoso Power Station of\n",
    "    J≈çban Joint Power Co., Ltd. \"\n",
    "    ]\n",
    "\n",
    "    Only answer the query provided. Return only one JSON answer/justification pair.\n",
    "    Query: {query_str}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    custom_RAG_template = PromptTemplate(\n",
    "        template=qa_prompt_custom_string\n",
    "    )\n",
    "\n",
    "    query_engine.update_prompts(\n",
    "        {\"response_synthesizer:text_qa_template\": custom_RAG_template}\n",
    "    )\n",
    "\n",
    "    display_and_run_prompt(query_engine, query_str, show_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_and_query_documents(documents, storage_context, query):\n",
    "    \"\"\"\n",
    "    Sets up vector store index to query against for a RAG pattern.\n",
    "    \"\"\"\n",
    "    #Using gemini embedding models\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    #Set up a query engine\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "\n",
    "    print(colored(\"*******Option 1: LlamaIndex Built-In Prompt*******\", \"red\"))\n",
    "    llama_built_in_prompt(query_engine, query, show_prompt = False)\n",
    "    print(colored(\"*******Option 2: LangChain Template RAG Prompt*******\", \"blue\"))\n",
    "    langchain_rag_prompt(query_engine, query, show_prompt = False)\n",
    "    print(colored(\"*******Option 3: Custom Few-Shot Prompt*******\", \"green\"))\n",
    "    custom_few_shot_prompt(query_engine,query, show_prompt=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RAG workflow for the llama-index built in prompt, templated LangChain prompt, and custom few-shot prompt\n",
    "\n",
    "query = \"what is minimum reserve rate of power?\"\n",
    "\n",
    "vector_idx = index_and_query_documents(docs, storage_context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Congratulations! You've implemented LlamaIndex on VertexAI for RAG applications with various types of prompting.\n",
    "\n",
    "Feel free to play around with the different input queries, prompt types, prompt structures and see how that impacts the output.\n",
    "\n",
    "Happy coding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
