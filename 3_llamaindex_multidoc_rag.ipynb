{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Leverage LLamaIndex with VertexAI Vector Search to perform question answering RAG\n",
    "\n",
    "## Multi-Document RAG\n",
    "\n",
    "So far we have established how to set up set up the necessary Google Cloud resources, set up a LlamaIndex agent, and customize prompts for RAG question answering. \n",
    "\n",
    "In this section we will cover Multi-Document Agents that can effectively answer different set of questions over a larger set of documents. Questions can include QA and summaries over a individual document or across documents. \n",
    "\n",
    "To do this we will follow these steps:\n",
    "\n",
    "+ setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc\n",
    "+ setup a top-level agent over this set of document agents: tool retrieval and answer over the set of tools responses to answer a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    SummaryIndex\n",
    ")\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\" #TODO - add your project-id here from the console\n",
    "REGION = \"\"  #TODO - add your region here from the console\n",
    "GCS_BUCKET = \"llamaindex_gcs_bucket\"  # @param {type:\"string\"}\n",
    "DOC_FOLDER = \"./data\"\n",
    "\n",
    "vs_index_id = \"\" #TODO - add your vertexai search id from setup here\n",
    "vs_endpoint_id = \"\"  #TODO - add your vertexai search deployed endpoint id from setup here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_multi_document():\n",
    "    import os\n",
    "    doc_dict = {}\n",
    "\n",
    "    for filename in os.listdir(DOC_FOLDER):\n",
    "        doc_dict[filename] = SimpleDirectoryReader(\n",
    "            input_files=[os.path.join(DOC_FOLDER, filename)]\n",
    "        ).load_data()\n",
    "\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm_and_storage(vs_index, vs_endpoint):\n",
    "    \"\"\"\n",
    "    Initializes VertexAI Vector Store given a VertexAI Search index and deployed endpoint.\n",
    "    Configures embedding and LLMs models to be gecko and Gemini.\n",
    "    \"\"\"\n",
    "    # setup storage\n",
    "    vector_store = VertexAIVectorStore(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        index_id=vs_index,\n",
    "        endpoint_id=vs_endpoint,\n",
    "        gcs_bucket_name=GCS_BUCKET,\n",
    "    )\n",
    "\n",
    "    # set storage context\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    gemini_embedding_model = VertexTextEmbedding(\"text-embedding-004\")\n",
    "    llm = Vertex(\"gemini-pro\")\n",
    "\n",
    "    Settings.embed_model = gemini_embedding_model\n",
    "    Settings.llm=llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents \n",
    "storage_context = initialize_llm_and_storage(vs_index_id, vs_endpoint_id)\n",
    "multi_docs = ingest_multi_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Document Level Agents\n",
    "\n",
    "First we will build document agents for each document. \n",
    "\n",
    "We will create two query engines, one for semantic search and one for summarization, for each document. These query engines will be converted into tools that can be passed to a function calling agent.\n",
    "\n",
    "We will be using the ReAct agent (short for \"Reasoning and Acting\"). This agent is an LLM-powered agent designed to perform complex tasks over your data. It operates in both \"read\" and \"write\" modes, making it a versatile tool for various applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_document_level_agents(documents, storage_context):\n",
    "    \"\"\"\n",
    "    Sets up a vector search and summarization tool for each document. Generates an agent for each documents based on tools.\n",
    "    \"\"\"\n",
    "\n",
    "    node_parser = SentenceSplitter()\n",
    "\n",
    "    #Build agents dictionary\n",
    "    agents = {}\n",
    "    query_engines = {}\n",
    "\n",
    "    for idx, doc_title in enumerate(documents):\n",
    "        \n",
    "        #A Node represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.\n",
    "        nodes = node_parser.get_nodes_from_documents(documents[doc_title], show_progress=True)\n",
    "\n",
    "        #Build query index\n",
    "        vector_index = VectorStoreIndex.from_documents(\n",
    "            documents[doc_title], storage_context=storage_context\n",
    "        )\n",
    "        #Build summary index\n",
    "        summary_index = SummaryIndex(nodes)\n",
    "\n",
    "        #Define engines\n",
    "        vector_query_engine = vector_index.as_query_engine()\n",
    "        summary_query_engine = summary_index.as_query_engine()\n",
    "\n",
    "        #Define tools\n",
    "        query_engine_tools = [\n",
    "            QueryEngineTool(\n",
    "                query_engine=vector_query_engine,\n",
    "                metadata=ToolMetadata(\n",
    "                    name=\"vector_tool\",\n",
    "                    description=(\n",
    "                        \"Useful for questions related to specific aspects of\"\n",
    "                        f\" {doc_title}.\"\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "            QueryEngineTool(\n",
    "                query_engine=summary_query_engine,\n",
    "                metadata=ToolMetadata(\n",
    "                    name=\"summary_tool\",\n",
    "                    description=(\n",
    "                        \"Useful for any requests that require a holistic summary\"\n",
    "                        f\" of EVERYTHING about {doc_title}. For questions about\"\n",
    "                        \" more specific sections, please use the vector_tool.\"\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        #Build agent\n",
    "        llm = Vertex(\"gemini-pro\")\n",
    "        agent = ReActAgent.from_tools(\n",
    "            query_engine_tools,\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            system_prompt=f\"\"\"\\\n",
    "            You are a specialized agent designed to answer queries about {doc_title}.\n",
    "            You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "            \"\"\",\n",
    "            )\n",
    "\n",
    "        agents[doc_title] = agent\n",
    "        query_engines[doc_title] = vector_index.as_query_engine(\n",
    "            similarity_top_k=2\n",
    "        )\n",
    "\n",
    "    return agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = build_document_level_agents(multi_docs, storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Top Level Agent\n",
    "\n",
    "We build a top-level agent that can orchestrate across the different document agents to answer any user query. This agent takes in all document agents as tools that were built above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_top_level_agent(agents):\n",
    "    #This agent takes in all document agents as tools\n",
    "    all_tools = []\n",
    "\n",
    "    for filename in os.listdir(DOC_FOLDER):\n",
    "        summary = (\n",
    "        f\"This content contains a research paper articles about {filename}. Use\"\n",
    "        f\" this tool if you want to answer any questions about {filename}.\\n\"\n",
    "        )\n",
    "        doc_tool = QueryEngineTool(\n",
    "            query_engine=agents[filename],\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"tool_{filename}\".rstrip(\".pdf\"),\n",
    "                description=summary,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        all_tools.append(doc_tool)\n",
    "\n",
    "\n",
    "    #define an \"object\" index and retriever over these tools\n",
    "    obj_index = ObjectIndex.from_objects(\n",
    "        all_tools,\n",
    "        index_cls=VectorStoreIndex,\n",
    "    )   \n",
    "\n",
    "    #Create top level agent\n",
    "    top_agent = ReActAgent.from_tools(\n",
    "        tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n",
    "        system_prompt=\"\"\" \\\n",
    "            You are an agent designed to answer queries about energy systems.\n",
    "            Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "            \"\"\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    return top_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_agent = build_top_level_agent(agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Multi-Document QA Rag\n",
    "\n",
    "Know we can query the top level agent. We will experiment with various question types that require lookup in individual documents and across multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA over a specific doc\n",
    "response = top_level_agent.query(\"Tell me about the Higashi-Shimizu Substation of Chubu Electric Power Grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries across documents \n",
    "response = top_level_agent.query(\"What are all projects introduced after the Great East Japan Earthquake?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross document QA\n",
    "response = top_level_agent.query(\"List out all the technologies that are used to stabilize the power system?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = top_level_agent.query(\"Explain to me what the building blocks of the MACH control and protection system and where it is used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have now built a multi document RAG workflow using LlamaIndex on VertexAI.\n",
    "\n",
    "You leveraged using by buildings tools that can perform summarization and semantic search for each document. Then using a ReAct agent for each document that uses the tools to perform complex tasks over the data.\n",
    "\n",
    "Then we built a top-level agent that can leverage all the document level agents information to answer complex questions that may span different documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
