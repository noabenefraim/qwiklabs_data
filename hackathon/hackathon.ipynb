{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Murder Mystery Hackathon \n",
    "\n",
    "Calling all hackers, sleuths, and tech enthusiasts! The brilliant mind behind the next generation of AI, Emon Lusk 2.0, has been found dead in his high-tech fortress. The police are baffled, the evidence is encrypted, and the clock is ticking.\n",
    "\n",
    "Join us for an exhilarating hackathon where you'll dive deep into the digital trails left behind by Lusk 2.0. Decipher cryptic codes, analyze AI logs, and uncover hidden connections to identify the killer.\n",
    "\n",
    "Will you expose a rival CEO's dark secret, unmask the AI's sinister role, or unveil a shocking betrayal from within? The truth lies buried in the data, waiting for you to unearth it.\n",
    "\n",
    "Gear up for a challenge that will test your technical skills, your analytical prowess, and your ability to collaborate under pressure. The fate of justice rests in your hands.\n",
    "\n",
    "Are you ready to crack the code and solve the Emon Lusk 2.0 murder mystery?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Your team will be acting as the lead investigators for this case. __Your task is to develop a presentation or report that summarizes your findings and presents a compelling case for the identity of the killer. Use your RAG skills to find the relevant information and data to help make your case by changing customizing the RAG pipeline with different prompts and/or indexing and retrieval methods.__\n",
    "\n",
    "We will provide you with the data sources and evidence needed.\n",
    "\n",
    "Here are some general investigative questions to help you get started with your prompting:\n",
    "\n",
    "+ Who had the motive to kill Emon Lusk 2.0?\n",
    "+ Analyze the relationships and potential conflicts between Lusk 2.0 and other characters (rival CEO, close associates, etc.).\n",
    "+ How was the murder carried out?\n",
    "+ Examine the crime scene details and autopsy report for clues about the murder weapon and method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "1. article_1.txt\n",
    "    + Title: News Article Series: Clues and Artifacts for a Murder Mystery Hackathon\n",
    "    + Blurb: Tech mogul Emon Lusk 2.0 was found dead in his Seattle mansion from a gunshot wound. The lack of forced entry and a mysterious encrypted message on his computer have baffled investigators. His death has shocked the tech world, and the search is on for the killer, with motives ranging from professional rivalry to personal vendetta.\n",
    "\n",
    "2. article_2.txt:\n",
    "    + Title: AI Assistant 'JARVIS' Questioned in Lusk 2.0 Murder Investigation\n",
    "    + Blurb: Investigators are examining JARVIS, the AI assistant of Emon Lusk 2.0, as a potential suspect in his murder. Anomalies in JARVIS's data logs, including gaps and unusual commands, have raised questions about its role, prompting experts to analyze whether it was a malfunction, manipulation, or something more sinister.\n",
    "\n",
    "3. article_3.txt\n",
    "    + Title: Rival Tech CEO Emerges as Person of Interest in Lusk 2.0 Case\n",
    "    + Blurb: A rival tech CEO has emerged as a person of interest in the Emon Lusk 2.0 murder case, due to their fierce competition and a potential motive related to Lusk 2.0's latest AI project. Investigators are facing challenges in building a case against the CEO, who is known for his meticulousness and ability to cover his tracks.\n",
    "\n",
    "4. article_4.txt\n",
    "    + Title: Cracking the Code: Cryptic Clues Emerge in Lusk 2.0 Murder Probe\n",
    "    + Blurb:Cryptographers have deciphered a hidden code in an encrypted message found on Emon Lusk 2.0's computer. The decoded message contains cryptic clues that may lead to the identification of his killer or their motive. Investigators are working with experts to fully unravel the meaning of the code and believe it holds the key to solving the murder.\n",
    "\n",
    "5. article_5.txt\n",
    "    + Title: Missing Piece of the Puzzle Found, Leading to Unexpected Twist  \n",
    "    + Blurb: A vintage pocket watch with a cryptic inscription led detectives to a hidden laboratory in Emon Lusk 2.0’s mansion. Evidence found in the lab implicates a close associate in the murder, revealing a shocking betrayal. The motive seems to be linked to Lusk 2.0's groundbreaking AI project.\n",
    "\n",
    "6. jarvis_data.txt\n",
    "    + Overview: This data source provides a detailed timeline of events leading up to and following the murder of Emon Lusk 2.0, as recorded by his AI assistant JARVIS. It includes logs of system activities, Lusk 2.0's actions, visitor access, and security alerts. Importantly, it highlights anomalies like gaps in data and an attempted security override, which could be crucial clues in the investigation.\n",
    "\n",
    "7. police_investigation.txt\n",
    "    + Overview: This data source offers a combination of physical and digital evidence found in a secret laboratory and through communication records. It reveals Lusk 2.0’s concerns about his AI project “Prometheus,” a potential motive for the close associate, and suspicious activity linked to the associate. It suggests the killer may have used Lusk 2.0's biometric data to access the lab and tamper with the project, leading to his death. The attempted security override and the masked individual at the library might be misdirections.\n",
    "\n",
    "8. voicenote.txt\n",
    "    + Overview: This data source provides a snippet of a heated conversation between Emon Lusk 2.0 and his associate two days before the murder. The conversation reveals a disagreement about the \"Prometheus\" project, with Lusk expressing concerns about its safety and the associate pushing for a faster release. It highlights the associate's ambition and potential motive for taking control of the project, even hinting at a willingness to act against Lusk 2.0's wishes. This conversation is crucial evidence pointing towards the associate's possible involvement in the murder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "Now it is time to put your rag skills to the test. As we learned earlier there are a few key components of RAG.\n",
    "\n",
    "__Loading:__ this refers to getting your data from where it lives -- whether it's text files, PDFs, another website, a database, or an API -- into your workflow. LlamaHub provides hundreds of connectors to choose from.\n",
    "\n",
    "__Indexing:__ this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "__Storing:__ once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "\n",
    "__Querying:__ for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "__Evaluation:__ a critical step in any flow is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a provided outline for each of these steps. Your task is to complete the RAG workflow leverage LlamaIndex. Feel free to play around with the prompts, the indexing, and different querying techniques.\n",
    "\n",
    "For each step there are options to customize your methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index \\\n",
    "  llama-index-embeddings-vertex \\\n",
    "  llama-index-llms-vertex \\\n",
    "  llama-index-core -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    Settings, \n",
    "    SimpleDirectoryReader\n",
    ")\n",
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    KeywordExtractor\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "credentials = google.auth.default()[0]\n",
    "request = google.auth.transport.requests.Request()\n",
    "credentials.refresh(request)\n",
    "\n",
    "# credentials will now have an api token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_embedding_model = VertexTextEmbedding(\"text-embedding-004\", credentials = credentials)\n",
    "llm = Vertex(model=\"gemini-pro\", temperature=0.0, max_tokens=500, credentials = credentials)\n",
    "\n",
    "Settings.embed_model = gemini_embedding_model\n",
    "Settings.llm=llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 1: \n",
    "Load to the data \n",
    "Customizable options: change the ingestion function - https://docs.llamaindex.ai/en/stable/understanding/loading/loading/\n",
    "'''\n",
    "documents = SimpleDirectoryReader(\"./hackathon_data\").load_data()\n",
    "\n",
    "# Customizable options: change the ingestion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2 and 3: Index the data and store the data\n",
    "Customizable options: add embeddings, create metadata from each article , etc\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction/\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/\n",
    "https://docs.llamaindex.ai/en/stable/examples/embeddings/gemini/\n",
    "\n",
    "'''\n",
    "vector_store = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Sample code for metadata creation and embeddings - customize based on your use case\n",
    "extractors = [\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    KeywordExtractor(keywords=10, llm=llm)]\n",
    "\n",
    "#Run metadata transformation pipeline.\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=extractors,\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=documents, in_place=False)\n",
    "\n",
    "print(nodes[1].metadata)\n",
    "\n",
    "#Generate embeddings for each metadata node\n",
    "for node in nodes:\n",
    "    node_embedding = gemini_embedding_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "\n",
    "vector_store.add(nodes)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: Query the data \n",
    "Customizable options - update the prompt of the query engine from the lab portion, change similarity top k, etc\n",
    "'''\n",
    "\n",
    "def print_output(response):\n",
    "    print(\"Response:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(response.response)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Source Documents:\")\n",
    "    print(\"-\" * 80)\n",
    "    for source in response.source_nodes:\n",
    "        print(f\"Sample Text: {source.text[:100]}\")\n",
    "        print(f\"Relevance score: {source.get_score():.3f}\")\n",
    "        print(f\"File Name: {source.metadata.get('file_name')}\")\n",
    "        print(f\"Page #: {source.metadata.get('page_label')}\")\n",
    "        print(f\"File Path: {source.metadata.get('file_path')}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "query_engine = vector_store.as_query_engine(\n",
    "    similarity_top_k = 2\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"insert your prompt here...\")\n",
    "print_output(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
